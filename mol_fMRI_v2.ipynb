{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Honor Code:\n",
    "    @author: Naman Jain\n",
    "    @github: njainmpi\n",
    "    date: 06 jan, 2026\n",
    "    Description: \"analysing fmri data to generate signal change map.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Importing python dependencies and in-house made functions to be used throughout the analysis pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nipype.interfaces import afni, fsl\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import os, datetime\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import re\n",
    "import getpass\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipyfilechooser import FileChooser\n",
    "from nilearn import image, plotting\n",
    "import scipy.stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import sqlite3\n",
    "from tabulate import tabulate\n",
    "from io import StringIO\n",
    "\n",
    "#Prime functions, can be choosen to run in the next cell\n",
    "def smooth_movavg(in_file, out_file, win_sec_duration, tr):\n",
    "\n",
    "  win = max(1, int(round(win_sec_duration / tr)))\n",
    "  \n",
    "  # Apply moving average\n",
    "  def moving_average_1d(x, win):\n",
    "      k = np.ones(win, dtype=float) / win\n",
    "      xpad = np.pad(x, (win//2, win-1-win//2), mode='edge')  # reduce edge shrinkage\n",
    "      return np.convolve(xpad, k, mode='valid')\n",
    "\n",
    "  img = nib.load(in_file)\n",
    "  data = img.get_fdata()   # X,Y,Z,T\n",
    "  T = data.shape[-1]\n",
    "  flat = data.reshape(-1, T)\n",
    "  sm = np.vstack([moving_average_1d(ts, win) for ts in flat]).reshape(data.shape)\n",
    "\n",
    "  nib.Nifti1Image(sm, img.affine, img.header).to_filename(out_file)\n",
    "  print(f\"Wrote: {out_file}  (tr={tr}s, window={win_sec_duration}s => {win} vols)\")\n",
    "def bruker_to_nifti(in_path, scan_number, out_file):\n",
    "    \n",
    "    scan_dir = os.path.join(in_path, scan_number)\n",
    "    method_file = os.path.join(scan_dir, \"method\")\n",
    "\n",
    "    # ---------- 1) Run brkraw tonii ----------\n",
    "    cmd = [\"brkraw\", \"tonii\", f\"{in_path}/\", \"-s\", str(scan_number)]\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "    # ---------- 2) Detect echo count in \"method\" ----------\n",
    "    if \"PVM_NEchoImages\" in open(method_file).read():\n",
    "        # Extract number of echoes using awk logic in Python\n",
    "        with open(method_file) as f:\n",
    "            for line in f:\n",
    "                if \"PVM_NEchoImages=\" in line:\n",
    "                    # Extract numeric part exactly like Bash substring 20..21\n",
    "                    echo_str = line.split(\"=\")[1].strip()\n",
    "                    NoOfEchoImages = int(echo_str)\n",
    "                    break\n",
    "\n",
    "        # ---------- 3) If single echo ----------\n",
    "        if NoOfEchoImages == 1:\n",
    "            src_files = glob.glob(f\"*{scan_number}*\")\n",
    "            for src in src_files:\n",
    "                shutil.copy(src, \"G1_cp.nii.gz\")\n",
    "\n",
    "        # ---------- 4) Multi-echo: merge then copy ----------\n",
    "        else:\n",
    "            merged_file = f\"{scan_number}_combined_images\"\n",
    "            src_files = glob.glob(f\"*{scan_number}*\")\n",
    "            # fslmerge -t combined.nii.gz file1 file2 file3 ...\n",
    "            subprocess.run([\"fslmerge\", \"-t\", merged_file] + src_files, check=True)\n",
    "            shutil.copy(f\"{merged_file}.nii.gz\", \"G1_cp.nii.gz\")\n",
    "\n",
    "    else:\n",
    "        # ---------- 5) No echo metadata ----------\n",
    "        src_files = glob.glob(f\"*{scan_number}*\")\n",
    "        for src in src_files:\n",
    "            shutil.copy(src, \"G1_cp.nii.gz\")\n",
    "\n",
    "    print(f\"{bcolors.NOTIFICATION}Fixing orientation to LPI{bcolors.ENDC}\")\n",
    "\n",
    "    # ---------- 6) Fix orientation to LPI using 3dresample ----------\n",
    "    resample = afni.Resample()\n",
    "    resample.inputs.in_file = \"G1_cp.nii.gz\"\n",
    "    resample.inputs.out_file = out_file\n",
    "    resample.inputs.orientation = \"LPI\"\n",
    "    resample.run()\n",
    "\n",
    "    # ---------- 7) Save NIfTI header info ----------\n",
    "    with open(\"NIFTI_file_header_info.txt\", \"w\") as out:\n",
    "        subprocess.run([\"fslhd\", out_file], stdout=out, check=True)\n",
    "\n",
    "    print_statement(f\"[OK] Bruker → NIFTI workflow completed.\", bcolors.OKGREEN)\n",
    "def extract_middle_volume(in_file, reference_vol, out_file, size):\n",
    "  extract_vol = fsl.ExtractROI()\n",
    "  extract_vol.inputs.in_file=in_file \n",
    "  extract_vol.inputs.t_min=reference_vol \n",
    "  extract_vol.inputs.t_size=size \n",
    "  extract_vol.inputs.roi_file=out_file\n",
    "  extract_vol.run()\n",
    "\n",
    "  print(\"[OK] Intended Volumes extracted.\")\n",
    "  return out_file\n",
    "def motion_correction(reference_vol, input_vol, output_prefix):\n",
    "\n",
    "    # ---------- 1) 3dvolreg ----------\n",
    "    \n",
    "    volreg = afni.Volreg()  \n",
    "    volreg.inputs.in_file = input_vol\n",
    "    volreg.inputs.basefile = reference_vol\n",
    "    volreg.inputs.out_file = f\"{output_prefix}.nii.gz\"\n",
    "    volreg.inputs.oned_file = \"motion.1D\"\n",
    "    volreg.inputs.args = '-linear'\n",
    "    volreg.inputs.oned_matrix_save = \"mats\"\n",
    "    volreg.inputs.oned_matrix_save = \"rmsabs.1D\"\n",
    "    volreg.inputs.verbose = True\n",
    "    volreg.run()\n",
    "\n",
    "    print(\"[INFO] Running 3dvolreg…\")\n",
    "    return output_prefix\n",
    "def plot_motion_parameters(input_file):\n",
    "\n",
    "    # ---------- 4) Plot motion parameters ----------\n",
    "    print(\"[INFO] Creating motion plots…\")\n",
    "\n",
    "    # Translation plots\n",
    "    data = np.loadtxt(input_file)\n",
    "\n",
    "    # If your file HAS a header, use:\n",
    "    # data = np.loadtxt(\"your_file.1D\", comments=\"#\")\n",
    "\n",
    "    # X-axis (row index / timepoints)\n",
    "    x = np.arange(data.shape[0])\n",
    "\n",
    "    # -------- Plot 1: first 3 columns --------\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i in range(3):\n",
    "        plt.plot(x, data[:, i], label=f\"Column {i+1}\")\n",
    "\n",
    "    plt.title(\"Rotation\")\n",
    "    plt.xlabel(\"Volume Number\")\n",
    "    plt.ylabel(\"Rotation in degrees\")\n",
    "    plt.legend([\"Pitch (x)\", \"Roll (y)\", \"Yaw (z)\"])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"motion_rotations.svg\", dpi=1200)\n",
    "    # -------- Plot 2: next 3 columns --------\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i in range(3, 6):\n",
    "        plt.plot(x, data[:, i], label=f\"Column {i+1}\")\n",
    "\n",
    "    plt.title(\"Translation\")\n",
    "    plt.xlabel(\"Volume Number\")\n",
    "    plt.ylabel(\"Translation in mm\")\n",
    "    plt.legend([\"Read (x)\", \"Phase (y)\", \"Slice (z)\"])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"motion_translations.svg\", dpi=1200)\n",
    "def compute_mean_range(input_file, prefix, start_idx, end_idx):\n",
    "    \n",
    "    afni_cmd = [\"3dTstat\", \"-mean\", \"-prefix\", prefix, f\"{input_file}[{start_idx}..{end_idx}]\"]\n",
    "\n",
    "    print(\"[INFO] Running:\", \" \".join(afni_cmd))\n",
    "\n",
    "    subprocess.run(afni_cmd, check=True)\n",
    "    print_statement(\"[OK] Mean baseline image saved.\", bcolors.OKGREEN)\n",
    "def masking_file(input_file, mask_file, output_file):\n",
    "    \n",
    "    math = fsl.maths.ApplyMask()\n",
    "    math.inputs.in_file = input_file\n",
    "    math.inputs.mask_file = mask_file\n",
    "    math.inputs.out_file = output_file\n",
    "\n",
    "    math.run()\n",
    "\n",
    "    print_statement(f\"[OK] Masked file saved → {output_file}\", bcolors.OKGREEN)\n",
    "    return output_file\n",
    "def tSNR(input_file, output_file, reference_vol, size):\n",
    "  \n",
    "  extract_middle_volume(input_file, reference_vol, \"extracted_ts.nii.gz\", size)\n",
    "\n",
    "  mean = fsl.maths.MeanImage()\n",
    "  mean.inputs.in_file = \"extracted_ts.nii.gz\"\n",
    "  mean.inputs.out_file = \"mean_image.nii.gz\"\n",
    "  mean.run()\n",
    "\n",
    "  std = fsl.maths.StdImage()\n",
    "  std.inputs.in_file = \"extracted_ts.nii.gz\"\n",
    "  std.inputs.out_file = \"std_image.nii.gz\"\n",
    "  std.run()\n",
    "\n",
    "  tSNR = fsl.maths.BinaryMaths()\n",
    "  tSNR.inputs.in_file = \"mean_image.nii.gz\"\n",
    "  tSNR.inputs.operand_file = \"std_image.nii.gz\"\n",
    "  tSNR.inputs.operation = \"div\"\n",
    "  tSNR.inputs.out_file = output_file\n",
    "  tSNR.run()\n",
    "\n",
    "  print_statement(f\"[OK] tSNR file saved → {output_file}\", bcolors.OKGREEN)\n",
    "  return output_file\n",
    "def spatial_smoothing(input_file, output_file, fwhm):\n",
    "  \n",
    "  smooth = fsl.maths.IsotropicSmooth()\n",
    "  smooth.inputs.in_file = input_file\n",
    "  smooth.inputs.out_file = output_file\n",
    "  # smooth.inputs.fwhm = fwhm\n",
    "  smooth.inputs.sigma = fwhm / 2.3548  # Convert FWHM to sigma\n",
    "\n",
    "  smooth.run()\n",
    "\n",
    "  print_statement(f\"[OK] Spatially smoothed file saved → {output_file}\", bcolors.OKGREEN)\n",
    "  return output_file\n",
    "def signal_change_map(signal_file, baseline_file, output_file):\n",
    "   \n",
    "  tmp_sub = \"tmp_signal_minus_baseline.nii.gz\"\n",
    "  tmp_div = \"tmp_psc_raw.nii.gz\"\n",
    "\n",
    "  sub = fsl.BinaryMaths()\n",
    "  sub.inputs.in_file = signal_file\n",
    "  sub.inputs.operand_file = baseline_file\n",
    "  sub.inputs.operation = \"sub\"\n",
    "  sub.inputs.out_file = tmp_sub\n",
    "  sub.run()\n",
    "\n",
    "  div = fsl.BinaryMaths()\n",
    "  div.inputs.in_file = tmp_sub\n",
    "  div.inputs.operand_file = baseline_file\n",
    "  div.inputs.operation = \"div\"\n",
    "  div.inputs.out_file = tmp_div\n",
    "  div.run()\n",
    "\n",
    "  mul = fsl.BinaryMaths()\n",
    "  mul.inputs.in_file = tmp_div\n",
    "  mul.inputs.operation = \"mul\"\n",
    "  mul.inputs.operand_value = 100\n",
    "  mul.inputs.out_file = output_file\n",
    "  mul.run()\n",
    "\n",
    "  os.remove(tmp_sub)\n",
    "  os.remove(tmp_div)\n",
    "\n",
    "  print_statement(f\"[OK] Percent Signal Change Map saved → {output_file}\", bcolors.OKGREEN)\n",
    "  return output_file\n",
    "def coregistration_afni(input_file1=None, input_file2=None, reference_file=None, output_file1=None, output_file2=None, estimate_affine=True, apply_affine=True, affine_mat=\"mean_func_struct_aligned.aff12.1D\"):\n",
    "\n",
    "  results = {}\n",
    "\n",
    "  # -------- STEP 1: Estimate affine --------\n",
    "  if estimate_affine:\n",
    "      if output_file1 is None:\n",
    "          raise ValueError(\"output_file1 must be provided when estimate_affine=True\")\n",
    "      if input_file1 is None:\n",
    "          raise ValueError(\"input_file1 must be provided when estimate_affine=True\")\n",
    "\n",
    "      coreg_wo_affine = afni.Allineate()\n",
    "      coreg_wo_affine.inputs.in_file = input_file1\n",
    "      coreg_wo_affine.inputs.reference = reference_file\n",
    "      coreg_wo_affine.inputs.out_matrix = affine_mat\n",
    "      coreg_wo_affine.inputs.cost = \"crU\"\n",
    "      coreg_wo_affine.inputs.two_pass = True\n",
    "      coreg_wo_affine.inputs.verbose = True\n",
    "      coreg_wo_affine.inputs.out_file = output_file1\n",
    "      coreg_wo_affine.inputs.out_param_file = \"params.1D\"\n",
    "      coreg_wo_affine.run()\n",
    "\n",
    "      print(f\"[OK] Affine estimated and saved → {affine_mat}\")\n",
    "      print(f\"[OK] Coregistered image (step 1) → {output_file1}\")\n",
    "\n",
    "      results[\"step1\"] = output_file1\n",
    "\n",
    "  # -------- STEP 2: Apply affine --------\n",
    "  if apply_affine:\n",
    "      if output_file2 is None:\n",
    "          raise ValueError(\"output_file2 must be provided when apply_affine=True\")\n",
    "      if input_file2 is None:\n",
    "          raise ValueError(\"input_file2 must be provided when apply_affine=True\")\n",
    "\n",
    "\n",
    "      coreg_with_affine = afni.Allineate()\n",
    "      coreg_with_affine.inputs.in_file = input_file2\n",
    "      coreg_with_affine.inputs.reference = reference_file\n",
    "      coreg_with_affine.inputs.in_matrix = affine_mat\n",
    "      coreg_with_affine.inputs.master = reference_file\n",
    "      coreg_with_affine.inputs.verbose = True\n",
    "      coreg_with_affine.inputs.final_interpolation = \"linear\"\n",
    "      coreg_with_affine.inputs.out_file = output_file2\n",
    "      coreg_with_affine.run()\n",
    "\n",
    "      print_statement(f\"[OK] Affine applied → {output_file2}\", bcolors.OKGREEN)\n",
    "      results[\"step2\"] = output_file2\n",
    "\n",
    "  return results\n",
    "def roi_analysis(func_in_file, roi_file, n_vols, tr, base_start, base_end, sig_start, sig_end):\n",
    "    print_statement(f\"Extracting time course for ROI: {roi_file}\", bcolors.NOTIFICATION)\n",
    "    output_file = f\"time_course_{roi_file.replace('.nii.gz', '.txt')}\"\n",
    "    time_course_extraction(roi_file, func_in_file, output_file)\n",
    "    print_statement(f\"[OK] Time course saved → {output_file}\", bcolors.OKGREEN)\n",
    "\n",
    "    #Creating Percent Signal Change graphs for each ROI\n",
    "    id_arr = list(range(0, n_vols, tr))\n",
    "    time_series = np.loadtxt(output_file)\n",
    "    # baseline = np.mean(time_series[base_start:base_end])\n",
    "    baseline = np.mean(time_series[base_start:base_end])\n",
    "    psc = ((time_series - baseline) / baseline) * 100\n",
    "    mean_signal = np.mean(psc[sig_start:sig_end])\n",
    "    print_statement(f\"[OK] Percent Signal Change calculated for ROI: {roi_file}\", bcolors.OKGREEN)\n",
    "    print(\"Time Series is:\", psc)\n",
    "    np.savetxt(f\"PSC_time_series_{roi_file.replace('.nii.gz', '.txt')}\", psc)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(id_arr, psc, label='Percent Signal Change')\n",
    "    plt.axvspan(base_start, base_end, color='green', alpha=0.3, label='Baseline Period')\n",
    "    plt.axvspan(sig_start, sig_end, color='blue', alpha=0.3, label='Signal Period')\n",
    "    plt.title(f'Percent Signal Change Time Series for {roi_file}')\n",
    "    plt.xlabel('Time Points (Volumes)')\n",
    "    plt.ylabel('MRI Signal Change (%)')\n",
    "    plt.set_ylim = (-2, 10)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    graph_file = f\"PSC_Time_Series_{roi_file.replace('.nii.gz', '.svg')}\"\n",
    "    plt.savefig(graph_file, dpi=1200)\n",
    "    print_statement(f\"[OK] Percent Signal Change graph saved → {graph_file}\", bcolors.OKGREEN)   \n",
    "\n",
    "    return mean_signal\n",
    "\n",
    "# Helper functions, compulsory to run\n",
    "def time_course_extraction(roi_file, func_file, output_file):\n",
    "   \n",
    "    ts = fsl.ImageMeants()\n",
    "    ts.inputs.in_file = func_file\n",
    "    ts.inputs.mask = roi_file\n",
    "    ts.inputs.out_file = output_file\n",
    "\n",
    "    ts.run()\n",
    "def func_param_extract(scan_dir, export_env=True):\n",
    "\n",
    "    scan_dir = Path(scan_dir)\n",
    "    acqp_file = scan_dir / \"acqp\"\n",
    "    method_file = scan_dir / \"method\"\n",
    "    \n",
    "\n",
    "    if not acqp_file.exists() or not method_file.exists():\n",
    "        raise FileNotFoundError(\"acqp or method file not found\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Read files\n",
    "    # -----------------------------\n",
    "    acqp_text = acqp_file.read_text()\n",
    "    method_text = method_file.read_text()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Sequence name (ACQ_protocol_name)\n",
    "    # -----------------------------\n",
    "    seq_match = re.search(\n",
    "        r\"ACQ_protocol_name=\\(\\s*64\\s*\\)\\s*\\n\\s*<([^>]+)>\",\n",
    "        acqp_text\n",
    "    )\n",
    "    SequenceName = seq_match.group(1) if seq_match else None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract numeric parameters\n",
    "    # -----------------------------\n",
    "    def get_value(pattern, text, cast=int):\n",
    "        m = re.search(pattern, text)\n",
    "        return cast(m.group(1)) if m else None\n",
    "\n",
    "    NoOfRepetitions = get_value(r\"##\\$PVM_NRepetitions=\\s*(\\d+)\", method_text)\n",
    "    TotalScanTime = get_value(r\"##\\$PVM_ScanTime=\\s*(\\d+)\", method_text)\n",
    "\n",
    "    Baseline_TRs = get_value(r\"PreBaselineNum=\\s*(\\d+)\", method_text)\n",
    "    StimOn_TRs = get_value(r\"StimNum=\\s*(\\d+)\", method_text)\n",
    "    StimOff_TRs = get_value(r\"InterStimNum=\\s*(\\d+)\", method_text)\n",
    "    NoOfEpochs = get_value(r\"NEpochs=\\s*(\\d+)\", method_text)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Derived values\n",
    "    # -----------------------------\n",
    "    VolTR_msec = None\n",
    "    VolTR = None\n",
    "    MiddleVolume = None\n",
    "\n",
    "    if NoOfRepetitions and TotalScanTime:\n",
    "        VolTR_msec = TotalScanTime / NoOfRepetitions\n",
    "        VolTR = VolTR_msec / 1000\n",
    "        MiddleVolume = NoOfRepetitions / 2\n",
    "\n",
    "    # -----------------------------\n",
    "    # Pack results\n",
    "    # -----------------------------\n",
    "    params = {\n",
    "        \"SequenceName\": SequenceName,\n",
    "        \"NoOfRepetitions\": NoOfRepetitions,\n",
    "        \"TotalScanTime\": TotalScanTime,\n",
    "        \"VolTR_msec\": VolTR_msec,\n",
    "        \"VolTR\": VolTR,\n",
    "        \"Baseline_TRs\": Baseline_TRs,\n",
    "        \"StimOn_TRs\": StimOn_TRs,\n",
    "        \"StimOff_TRs\": StimOff_TRs,\n",
    "        \"NoOfEpochs\": NoOfEpochs,\n",
    "        \"MiddleVolume\": MiddleVolume,\n",
    "    }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Export to environment (optional)\n",
    "    # -----------------------------\n",
    "    if export_env:\n",
    "        for k, v in params.items():\n",
    "            if v is not None:\n",
    "                os.environ[k] = str(v)\n",
    "\n",
    "    return params\n",
    "def extract_subject_id(scan_dir):\n",
    "    subject_file = Path(scan_dir) / \"subject\"\n",
    "\n",
    "    if not subject_file.exists():\n",
    "        raise FileNotFoundError(f\"'subject' file not found in {scan_dir}\")\n",
    "\n",
    "    with subject_file.open(\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().startswith(\"##$SUBJECT_id\"):\n",
    "            # The value is expected on the next line\n",
    "            value_line = lines[i + 1].strip()\n",
    "            return value_line.strip(\"<>\")\n",
    "\n",
    "    raise ValueError(\"##$SUBJECT_id not found in subject file\")\n",
    "def print_header(message, color):\n",
    "    line = \"*\" * 134   # same width everywhere\n",
    "    width = len(line)\n",
    "\n",
    "    print()\n",
    "    print(f\"{color}{line}{bcolors.ENDC}\")\n",
    "    print(f\"{color}{message.center(width)}{bcolors.ENDC}\")\n",
    "    print(f\"{color}{line}{bcolors.ENDC}\")\n",
    "    print()\n",
    "def print_statement(message, color):\n",
    "    print(f\"{color}{message}{bcolors.ENDC}\")\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    NOTIFICATION = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "def view_images(input_image, cmap=\"gray\"):\n",
    "    img = nib.load(input_image)\n",
    "    data = img.get_fdata()\n",
    "\n",
    "    # Handle 4D vs 3D safely\n",
    "    if data.ndim == 4:\n",
    "        data = data[..., 0]   # take first volume\n",
    "    elif data.ndim != 3:\n",
    "        raise ValueError(f\"Unsupported data shape: {data.shape}\")\n",
    "\n",
    "    ny = data.shape[1]\n",
    "    cols = 8\n",
    "    rows = int(np.ceil(ny / cols))\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "    axes = np.atleast_1d(axes).flatten()\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < ny:\n",
    "            ax.imshow(data[:, i, :].T, cmap=cmap, origin=\"lower\")\n",
    "            ax.set_title(f\"y={i}\", fontsize=14)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def matching_nifti_files(directory, ref_shape):\n",
    "    matches = []\n",
    "\n",
    "    ref_xyz = tuple(ref_shape[:3])  # (64,16,64)\n",
    "    print(f\"Reference spatial shape: {ref_xyz}\")\n",
    "\n",
    "    for f in sorted(os.listdir(directory)):\n",
    "        if f.endswith((\".nii\", \".nii.gz\")):\n",
    "            try:\n",
    "                img = nib.load(os.path.join(directory, f))\n",
    "                img_xyz = tuple(img.shape[:3])\n",
    "\n",
    "                if img_xyz == ref_xyz:\n",
    "                    matches.append(f)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {f}: {e}\")\n",
    "\n",
    "    \n",
    "    return matches\n",
    "def roi_analysis(func_in_file, roi_file, n_vols, tr, base_start, base_end, sig_start, sig_end):\n",
    "    print_statement(f\"Extracting time course for ROI: {roi_file}\", bcolors.NOTIFICATION)\n",
    "    output_file = f\"time_course_{roi_file.replace('.nii.gz', '.txt')}\"\n",
    "    time_course_extraction(roi_file, func_in_file, output_file)\n",
    "    print_statement(f\"[OK] Time course saved → {output_file}\", bcolors.OKGREEN)\n",
    "\n",
    "    #Creating Percent Signal Change graphs for each ROI\n",
    "    id_arr = list(range(0, n_vols, tr))\n",
    "    time_series = np.loadtxt(output_file)\n",
    "    # baseline = np.mean(time_series[base_start:base_end])\n",
    "    baseline = np.mean(time_series[base_start:base_end])\n",
    "    psc = ((time_series - baseline) / baseline) * 100\n",
    "    mean_signal = np.mean(psc[sig_start:sig_end])\n",
    "    print_statement(f\"[OK] Percent Signal Change calculated for ROI: {roi_file}\", bcolors.OKGREEN)\n",
    "    print(\"Time Series is:\", psc)\n",
    "    np.savetxt(f\"PSC_time_series_{roi_file.replace('.nii.gz', '.txt')}\", psc)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(id_arr, psc, label='Percent Signal Change')\n",
    "    plt.axvspan(base_start, base_end, color='green', alpha=0.3, label='Baseline Period')\n",
    "    plt.axvspan(sig_start, sig_end, color='blue', alpha=0.3, label='Signal Period')\n",
    "    plt.title(f'Percent Signal Change Time Series for {roi_file}')\n",
    "    plt.xlabel('Time Points (Volumes)')\n",
    "    plt.ylabel('MRI Signal Change (%)')\n",
    "    plt.set_ylim = (-2, 10)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    graph_file = f\"PSC_Time_Series_{roi_file.replace('.nii.gz', '.svg')}\"\n",
    "    plt.savefig(graph_file, dpi=1200)\n",
    "    print_statement(f\"[OK] Percent Signal Change graph saved → {graph_file}\", bcolors.OKGREEN)   \n",
    "\n",
    "    return mean_signal\n",
    "def int_widget(value, desc):\n",
    "    return widgets.IntText(\n",
    "        value=value,\n",
    "        description=desc,\n",
    "        layout=INPUT_LAYOUT,\n",
    "        style={'description_width': DESC_WIDTH}\n",
    "    )\n",
    "def print_selected_pipeline_steps():\n",
    "    print(\"\\nPIPELINE STEP SELECTION SUMMARY\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for step in PIPELINE_STEPS:\n",
    "        key = step[\"key\"]\n",
    "        name = step[\"name\"]\n",
    "\n",
    "        if checkboxes[key].value:\n",
    "            print(f\"[✓] {name}\")\n",
    "        else:\n",
    "            print(f\"[ ] {name}\")\n",
    "\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_STEPS = [\n",
    "    {\"name\": \"Bruker → NIfTI\", \"key\": \"bruker_to_nifti\", \"func\": bruker_to_nifti, \"inputs\": [\"in_path\", \"scan_number\"], \"produces\": \"func_file\"},\n",
    "    {\"name\": \"Motion Correction\", \"key\": \"motion_correction\", \"func\": motion_correction, \"inputs\": [\"reference_vol\", \"func_file\", \"output_prefix\"], \"produces\": \"func_file\"},\n",
    "    {\"name\": \"Temporal Smoothing (MovAvg)\", \"key\": \"smooth_movavg\", \"func\": smooth_movavg, \"inputs\": [\"func_file\", \"out_file\", \"win_sec_duration\", \"tr\"], \"produces\": \"func_file\"},\n",
    "    {\"name\": \"Spatial Smoothing\", \"key\": \"spatial_smoothing\", \"func\": spatial_smoothing, \"inputs\": [\"func_file\", \"out_file\", \"fwhm\"], \"produces\": \"func_file\"},\n",
    "    {\"name\": \"Compute Mean Baseline\", \"key\": \"compute_mean_range\", \"func\": compute_mean_range, \"inputs\": [\"func_file\", \"prefix\", \"start_idx\", \"end_idx\"], \"produces\": \"baseline_file\"},\n",
    "    {\"name\": \"Masking\", \"key\": \"masking_file\", \"func\": masking_file, \"inputs\": [\"func_file\", \"mask_file\", \"out_file\"], \"produces\": \"func_file\"},\n",
    "    {\"name\": \"Temporal SNR Estimation\", \"key\": \"tSNR\", \"func\": tSNR, \"inputs\": [\"func_file\", \"out_file\", \"reference_vol\", \"size\"], \"produces\": \"tsnr_file\"},\n",
    "    {\"name\": \"Signal Change Map\", \"key\": \"signal_change_map\", \"func\": signal_change_map, \"inputs\": [\"func_file\", \"baseline_file\", \"out_file\"], \"produces\": \"psc_file\"},\n",
    "    {\"name\": \"ROI Analysis\", \"key\": \"roi_analysis\", \"func\": roi_analysis, \"inputs\": [\"func_in_file\", \"roi_file\", \"n_vols\", \"tr\", \"base_start\", \"base_end\", \"sig_start\", \"sig_end\"], \"produces\": \"roi_graphs\"},\n",
    "    {\"name\": \"Coregistration\", \"key\": \"coregistration_afni\", \"func\": coregistration_afni, \"inputs\": [\"input_file1\", \"input_file2\", \"reference_file\"], \"produces\": \"roi_graphs\"}]\n",
    "\n",
    "checkboxes = {}\n",
    "\n",
    "for step in PIPELINE_STEPS:\n",
    "    cb = widgets.Checkbox(value=True, description=step[\"name\"], indent=False)\n",
    "    checkboxes[step[\"key\"]] = cb\n",
    "\n",
    "box = widgets.Box(list(checkboxes.values()), layout=widgets.Layout(display=\"flex\", flex_flow=\"row wrap\", align_items=\"flex-start\", gap=\"10px\"))\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_selected_pipeline_steps()\n",
    "def is_step_enabled(step_key):\n",
    "    return checkboxes.get(step_key, None) and checkboxes[step_key].value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Based on the root location, selecting the folder where raw data is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choosing the raw data that needs to be analysed\n",
    "root_location = \"/Volumes/Extreme_Pro/fMRI\"\n",
    "selected_folder = FileChooser(root_location, layout=widgets.Layout(width='1080px'))\n",
    "selected_folder.show_only_dirs = True\n",
    "display(selected_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = Path(selected_folder.selected_path)\n",
    "print(f\"Our Raw Data is located in the path {in_path}\")\n",
    "subject_id = extract_subject_id(in_path)\n",
    "\n",
    "# ---------- Common styles ----------\n",
    "INPUT_LAYOUT = widgets.Layout(width=\"280px\", flex=\"0 0 auto\")\n",
    "ROW_LAYOUT = widgets.Layout(width=\"100%\", justify_content=\"space-between\", align_items=\"center\", padding=\"12px\", border=\"3px solid #444\", border_radius=\"10px\", background_color=\"#1e1e1e\")\n",
    "DESC_WIDTH = \"220px\"\n",
    "\n",
    "# ---------- Widgets (KEEP REFERENCES) ----------\n",
    "func_scan_number_entered   = int_widget(10, \"Functional Scan:\")\n",
    "struct_scan_number_entered = int_widget(10, \"Structural Scan:\")\n",
    "win_entered         = int_widget(100, \"Window Duration (in vols):\")\n",
    "temp_smoothing_window = int_widget(60, \"Temporal Smoothing (in vols):\")\n",
    "\n",
    "row = widgets.HBox([func_scan_number_entered, struct_scan_number_entered, win_entered, temp_smoothing_window], layout=ROW_LAYOUT)\n",
    "display(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_scan_number = str(func_scan_number_entered.value)\n",
    "struct_scan_number = str(struct_scan_number_entered.value)\n",
    "win = str(win_entered.value)\n",
    "\n",
    "print_statement(f\"Subject under analysis is {subject_id} for Functional Scan Number {func_scan_number} and Structural Scan Number {struct_scan_number}\", bcolors.NOTIFICATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Creating and assigning directories to the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters extraction for structural and functional files\n",
    "\n",
    "params_struct = func_param_extract(Path(in_path) / struct_scan_number,export_env=True)\n",
    "sequence_name_struct = params_struct[\"SequenceName\"]\n",
    "\n",
    "params_func = func_param_extract(Path(in_path) / func_scan_number,export_env=True)\n",
    "sequence_name_func = params_func[\"SequenceName\"]\n",
    "\n",
    "\n",
    "# Setting up path for storing analysed data overall, strucutral data, functional data and processed functional data\n",
    "# Analysed data overall\n",
    "parts = list(in_path.parts)\n",
    "parts[parts.index(\"RawData\")] = \"AnalysedData\"\n",
    "analysed_base = Path(*parts).parent\n",
    "analysed_path = analysed_base / subject_id\n",
    "\n",
    "# Structural and Functional data\n",
    "analysed_struct_dir = Path(analysed_path) / f\"{struct_scan_number}{sequence_name_struct}\"\n",
    "analysed_func_dir = Path(analysed_path) / f\"{func_scan_number}{sequence_name_func}\"\n",
    "\n",
    "# processed functional data\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "user = getpass.getuser()\n",
    "folder_created = f\"{timestamp}_{user}\"\n",
    "analysis_dir = Path(analysed_path) / f\"{func_scan_number}{sequence_name_func}\" / folder_created\n",
    "analysis_dir.mkdir(parents=True, exist_ok=False)  # fail loudly if it already exists\n",
    "\n",
    "# src_dir = Path.cwd()\n",
    "\n",
    "# path for raw strucutral and functional data \n",
    "path_raw_struct = os.path.join(in_path, struct_scan_number)\n",
    "path_raw_func = os.path.join(in_path, func_scan_number)\n",
    "\n",
    "# Display of variables and paths in tabular form\n",
    "var_vals = [\n",
    "            ['Location of all Raw Data', 'root_location', root_location],\n",
    "            ['Location of Raw Data to be analysed', 'in_path', in_path],\n",
    "            ['Location of Raw Strcutural Data', 'path_raw_struct', path_raw_struct],\n",
    "            ['Location of Raw Functional Data', 'path_raw_func', path_raw_func],\n",
    "            ['Location of analysed structural data', 'analysed_struct_dir', analysed_struct_dir],\n",
    "            ['Location of analysed functional data', 'analysed_func_dir', analysed_func_dir],\n",
    "            ['Lcoation of final processed functional dta', 'analysis_dir', analysis_dir]\n",
    "            ]\n",
    "\n",
    "# pd.set_option('display.max_colwidth', 500)  # or 199\n",
    "df = pd.DataFrame(var_vals, columns=['Purpose', 'Variable Name', 'Value'], index=['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n",
    "print(tabulate(df, headers='keys', tablefmt='psql'))\n",
    "\n",
    "\n",
    "print_statement(\"For your information: all generated files after processing stored in the directory will be utilising the below nomenclature:\", bcolors.NOTIFICATION)\n",
    "\n",
    "print(f\"After applying motion correction, file will be saved as mc_input_file\")\n",
    "print(f\"After applying temporal smoothing, file will be saved as ts_input_file\")\n",
    "print(f\"After applying spatial smoothing, file will be saved as sm_input_file\")\n",
    "print(f\"After applying temporal smoothing, file will be saved as ts_input_file\")\n",
    "print(f\"After applying temporal SNR, file will be saved as tsnr_input_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Converting Functional Data into NIFTI and Processing Functional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_statement(f\"Analysed Data Path: {analysed_func_dir}\", bcolors.NOTIFICATION)\n",
    "analysed_func_dir.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(analysed_func_dir)\n",
    "\n",
    "if is_step_enabled(\"bruker_to_nifti\"):\n",
    "    print_header(\"Converting Bruker to NIFTI: Functional Data\", bcolors.HEADER)\n",
    "\n",
    "    nifti_file = analysed_func_dir / \"func.nii.gz\"\n",
    "\n",
    "    if nifti_file.exists():\n",
    "        print_statement(\"NIfTI file already exists. Skipping conversion.\", bcolors.OKGREEN)\n",
    "    else:\n",
    "        bruker_to_nifti(in_path, func_scan_number, \"func.nii.gz\")\n",
    "\n",
    "    input_name_for_next_step = \"func\"\n",
    "    \n",
    "else:\n",
    "    print_statement(\"⏭️ Data Conversion Step not executed, May cause trouble in running further analysis\", bcolors.NOTIFICATION)\n",
    "\n",
    "input_name_for_next_step = \"func\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Generating Masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = int(params_func[\"VolTR\"])\n",
    "n_vols = params_func[\"NoOfRepetitions\"]\n",
    "middle_vol = str(int(n_vols / 2))\n",
    "\n",
    "extract_middle_volume(f\"{input_name_for_next_step}.nii.gz\", int(middle_vol), \"middle_vol.nii.gz\", 1)\n",
    "\n",
    "#Creating a mask to be applied on functional data using the mean baseline image\n",
    "\n",
    "if is_step_enabled(\"masking_file\"):\n",
    "    \n",
    "    mask_file = \"mask_mean_mc_func.nii.gz\"\n",
    "    mask_file_cannulas = \"mask_mean_mc_func_cannulas.nii.gz\"\n",
    "    \n",
    "\n",
    "    if os.path.exists(mask_file):\n",
    "        print(f\"{bcolors.OKGREEN}Mask Image ({mask_file}) exists.{bcolors.ENDC}\")\n",
    "    else:\n",
    "        print(f\"{bcolors.FAIL}Mask Image does not exist. Please create the mask and save it as mask_mean_mc_func.nii.gz{bcolors.ENDC}\")\n",
    "        subprocess.run([\"fsleyes\", \"middle_vol.nii.gz\"])\n",
    "\n",
    "    if os.path.exists(mask_file_cannulas):\n",
    "        print(f\"{bcolors.OKGREEN}Mask Image including cannulas ({mask_file_cannulas}) exist.{bcolors.ENDC}\")\n",
    "    else:\n",
    "        print(f\"{bcolors.FAIL}Mask Image does not exist. Please create the mask that also includes cannulas and save it as mask_mean_mc_func_cannulas.nii.gz.{bcolors.ENDC}\")\n",
    "        shutil.copyfile(\"mask_mean_mc_func.nii.gz\", \"mask_mean_mc_func_cannulas.nii.gz\")\n",
    "        subprocess.run([\"fsleyes\", \"mean_image.nii.gz\" , \"mask_mean_mc_func_cannulas.nii.gz\"])\n",
    "else:\n",
    "    print_statement(\"⏭️ Masks not generated, will not get cleaned data in further pipeline\", bcolors.NOTIFICATION)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Applying Motion Correction on raw functional data and plotting motion parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_step_enabled(\"motion_correction\"):\n",
    "    print_header(\"Applying Motion Correction on raw functional data and plotting motion parameters\", bcolors.HEADER)\n",
    "\n",
    "    if os.path.exists(\"mc_func.nii.gz\"):\n",
    "        print(f\"{bcolors.OKGREEN}Motion Corrected functional data exists. Skipping motion correction.{bcolors.ENDC}\")\n",
    "    else:\n",
    "        motion_correction(\"middle_vol.nii.gz\", f\"{input_name_for_next_step}.nii.gz\", output_prefix=f\"mc_{input_name_for_next_step}\")\n",
    "        input_name_for_next_step = f\"mc_{input_name_for_next_step}\"\n",
    "        \n",
    "    #Display of the motion corrected image and motion corrected graphs\n",
    "    view_images(\"mc_func.nii.gz\", cmap=\"gray\")\n",
    "    plot_motion_parameters(\"motion.1D\")    \n",
    "else:\n",
    "    print_statement(\"⏭️ Motion Correction not executed\", bcolors.NOTIFICATION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Move into further subdirectory with in analysed data folder as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_name_for_next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_statement(f\"Analysed Data Path Final: {analysis_dir}\", bcolors.NOTIFICATION)\n",
    "analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(analysis_dir)\n",
    "\n",
    "mask_file = \"../mask_mean_mc_func.nii.gz\"\n",
    "mask_file_cannulas = \"../mask_mean_mc_func_cannulas.nii.gz\"\n",
    "\n",
    "shutil.copyfile(f\"../{input_name_for_next_step}.nii.gz\", f\"{input_name_for_next_step}.nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Applying temporal smoothing to the motion corrected functional data to see the temporal signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying temporal smoothing to the motion corrected functional data to see the temporal signatures\n",
    "\n",
    "if is_step_enabled(\"smooth_movavg\"):\n",
    "    print_header(\"Applying temporal smoothing to the motion corrected functional data to see the temporal signatures\", bcolors.HEADER)\n",
    "    smooth_movavg(f\"{input_name_for_next_step}.nii.gz\", f\"ts_{input_name_for_next_step}.nii.gz\", temp_smoothing_window.value, tr)\n",
    "\n",
    "    input_name_for_next_step = f\"ts_{input_name_for_next_step}\"\n",
    "\n",
    "    if is_step_enabled(\"masking_file\"):\n",
    "        masking_file(f\"{input_name_for_next_step}.nii.gz\", mask_file, f\"cleaned_{input_name_for_next_step}.nii.gz\")\n",
    "        masking_file(f\"{input_name_for_next_step}.nii.gz\", mask_file_cannulas, f\"cleaned_{input_name_for_next_step}_with_cannulas.nii.gz\")\n",
    "        masking_file(\"../middle_vol.nii.gz\", mask_file_cannulas, \"cleaned_mean_image_with_cannulas.nii.gz\")\n",
    "        mean_image_for_coregistrataion = \"cleaned_mean_image_with_cannulas.nii.gz\"\n",
    "        func_file_with_cannulas = f\"cleaned_{input_name_for_next_step}_with_cannulas.nii.gz\"\n",
    "    else:\n",
    "        mean_image_for_coregistrataion = \"mean_image.nii.gz\"\n",
    "\n",
    "else:\n",
    "    print_statement(\"⏭️ Temporal Smoothing not executed\", bcolors.NOTIFICATION)\n",
    "    func_file_without_cannulas = f\"ts_{input_name_for_next_step}.nii.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening fsleyes to view the temporally smoothed motion corrected functional data\n",
    "print_statement(\"Choose your baseline and signal volumes from the temporally smoothed motion corrected functional data.\", bcolors.NOTIFICATION)\n",
    "subprocess.run([\"fsleyes\", f\"{input_name_for_next_step}.nii.gz\"])\n",
    "\n",
    "# ---------- Common styling ----------\n",
    "input_layout = widgets.Layout(width=\"280px\", flex=\"0 0 auto\")\n",
    "label_style = {'description_width': '410px'}\n",
    "\n",
    "# ---------- Widgets ----------\n",
    "base_start_idx = widgets.IntText(value=10, description=\"Baseline Start Index:\", layout=input_layout, style=label_style)\n",
    "sig_start_idx = widgets.IntText(value=10, description=\"Signal Start Index:\", layout=input_layout, style=label_style)\n",
    "\n",
    "for w in [base_start_idx, sig_start_idx]:\n",
    "    w.style.description_width = \"180px\"\n",
    "\n",
    "# ---------- Container ----------\n",
    "row = widgets.HBox([base_start_idx, sig_start_idx],\n",
    "    layout=widgets.Layout(width=\"40%\", justify_content=\"space-between\", align_items=\"center\", padding=\"12px\", border=\"3px solid #444\", border_radius=\"10px\", background_color=\"#1e1e1e\"))\n",
    "\n",
    "row.layout.background_color = \"#1e1e1e\"\n",
    "row.layout.border = \"3px solid #444\"\n",
    "\n",
    "display(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_start = int(base_start_idx.value)\n",
    "sig_start  = int(sig_start_idx.value)\n",
    "base_end   = base_start + int(win_entered.value)\n",
    "sig_end   = sig_start + int(win_entered.value)\n",
    "\n",
    "print_statement(f\"Your baseline window selected is: {base_start}_to_{base_end}\", bcolors.OKGREEN)\n",
    "print_statement(f\"Your signal window selected is: {sig_start}_to_{sig_end}\", bcolors.OKGREEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# tSNR estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimating tSNR using the cleaned motion corrected functional data\n",
    "\n",
    "if is_step_enabled(\"tSNR\"):\n",
    "    print_header(\"Estimating tSNR using the cleaned motion corrected functional data\", bcolors.HEADER)\n",
    "    tSNR(input_file=f\"{input_name_for_next_step}.nii.gz\", reference_vol=100, output_file=f\"tSNR_{input_name_for_next_step}.nii.gz\", size=400)\n",
    "    view_images(f\"tSNR_{input_name_for_next_step}.nii.gz\", cmap=\"viridis\")\n",
    "\n",
    "else:\n",
    "    print_statement(\"⏭️ Temporal SNR executed\", bcolors.NOTIFICATION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Applying spatial smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying isotropic spatial smoothing on cleaned motion corrected functional data\n",
    "\n",
    "if is_step_enabled(\"spatial_smoothing\"):\n",
    "    \n",
    "    fwhm_kernel = float(0.7)\n",
    "    \n",
    "    print_header(\"Applying isotropic spatial smoothing\", bcolors.HEADER)\n",
    "    spatial_smoothing(f\"{input_name_for_next_step}.nii.gz\", f\"sm_{input_name_for_next_step}.nii.gz\", fwhm_kernel)\n",
    "    \n",
    "    input_name_for_next_step = f\"sm_{input_name_for_next_step}\"\n",
    "\n",
    "    if is_step_enabled(\"masking_file\"):\n",
    "        masking_file(f\"{input_name_for_next_step}.nii.gz\", mask_file, f\"cleaned_{input_name_for_next_step}\")\n",
    "        view_images(f\"cleaned_{input_name_for_next_step}.nii.gz\", cmap=\"gray\")\n",
    "else:\n",
    "    print_statement(\"⏭️ Spatial Smoothing not executed\", bcolors.NOTIFICATION)\n",
    "    view_images(f\"{input_name_for_next_step}.nii.gz\", cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# Generating Signal Change Maps and Signal Change Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Signal Change Map\n",
    "\n",
    "if is_step_enabled(\"signal_change_map\"):\n",
    "\n",
    "    print_header(\"Generating Signal Change Map and Signal Change Time Series\", bcolors.HEADER)\n",
    "\n",
    "    compute_mean_range(input_file=f\"{input_name_for_next_step}.nii.gz\", prefix=f\"mean_baseline_image_{base_start}_to_{base_end}.nii.gz\", start_idx=base_start, end_idx=base_end)\n",
    "    compute_mean_range(input_file=f\"{input_name_for_next_step}.nii.gz\", prefix=f\"mean_signal_image_{sig_start}_to_{sig_end}.nii.gz\", start_idx=sig_start, end_idx=sig_end)\n",
    "\n",
    "    signal_change_map(f\"mean_signal_image_{sig_start}_to_{sig_end}.nii.gz\", f\"mean_baseline_image_{base_start}_to_{base_end}.nii.gz\", f\"scm_from_{input_name_for_next_step}.nii.gz\") # Creating SCM image\n",
    "    signal_change_map(f\"{input_name_for_next_step}.nii.gz\", f\"mean_baseline_image_{base_start}_to_{base_end}.nii.gz\", f\"scm_timeseries_from_{input_name_for_next_step}.nii.gz\") # Creating Signal Change Time Series\n",
    "    view_images(f\"scm_from_{input_name_for_next_step}.nii.gz\", cmap=\"inferno\")\n",
    "    uncleaned_scm_file = f\"scm_from_{input_name_for_next_step}.nii.gz\"\n",
    "\n",
    "    if is_step_enabled(\"masking_file\"):\n",
    "        masking_file(f\"scm_from_{input_name_for_next_step}.nii.gz\", mask_file, f\"cleaned_scm_from_{input_name_for_next_step}.nii.gz\")\n",
    "        masking_file(f\"scm_timeseries_from_{input_name_for_next_step}.nii.gz\", mask_file, f\"cleand_scm_timeseries_from_{input_name_for_next_step}.nii.gz\")\n",
    "        cleaned_scm_file = f\"cleaned_scm_from_{input_name_for_next_step}.nii.gz\"\n",
    "else:\n",
    "    print_statement(\"⏭️ Signal Change Map not estimated\", bcolors.NOTIFICATION)\n",
    "\n",
    "print(uncleaned_scm_file, cleaned_scm_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# Converting Structural Data into NIFTI and Processing Structural Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_statement(f\"Analysed Data Path: {analysed_struct_dir}\", bcolors.NOTIFICATION)\n",
    "analysed_struct_dir.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(analysed_struct_dir)\n",
    "\n",
    "if is_step_enabled(\"bruker_to_nifti\"):\n",
    "    print_header(\"Converting Bruker to NIFTI: Structural Data\", bcolors.HEADER)\n",
    "\n",
    "    nifti_file = analysed_struct_dir / \"struct.nii.gz\"\n",
    "    if nifti_file.exists():\n",
    "        print_statement(\"NIfTI file already exists. Skipping conversion.\", bcolors.OKGREEN)\n",
    "    else:\n",
    "        bruker_to_nifti(in_path, struct_scan_number, \"struct.nii.gz\")\n",
    "\n",
    "else:\n",
    "    print_statement(\"⏭️ Data Conversion Step not executed, May cause trouble in running further analysis\", bcolors.NOTIFICATION)\n",
    "\n",
    "\n",
    "#Cleaning the structural image by masking it with a manually created mask\n",
    "\n",
    "if is_step_enabled(\"masking_file\"):\n",
    "    print_statement(\"Cleaning the structural image by manually creating mask\", bcolors.NOTIFICATION)\n",
    "    \n",
    "    if os.path.exists(\"cleaned_struct.nii.gz\"):\n",
    "        print_statement(\"Structural Image for Coregistration exists.\", bcolors.OKGREEN)\n",
    "    else:\n",
    "        print_statement(\"Please create a mask for the structural image and save it as mask_anatomy.nii.gz\", bcolors.NOTIFICATION)\n",
    "        subprocess.run([\"fsleyes\", \"struct.nii.gz\"])\n",
    "        masking_file(\"struct.nii.gz\", \"mask_struct.nii.gz\", \"cleaned_struct.nii.gz\")\n",
    "\n",
    "    structural_file_for_coregistration = os.path.join(analysed_struct_dir, \"cleaned_struct.nii.gz\")    \n",
    "    view_images(\"cleaned_struct.nii.gz\", cmap=\"gray\")\n",
    "\n",
    "else:\n",
    "    print_statement(\"⏭️ Masks not generated, will not get cleaned data in further pipeline\", bcolors.NOTIFICATION)    \n",
    "    structural_file_for_coregistration = os.path.join(analysed_struct_dir, \"struct.nii.gz\")    \n",
    "    view_images(\"struct.nii.gz\", cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "# Coregistering functional time series and functional signal change map to structural image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_statement(f\"Analysed Data Path: {analysed_func_dir}\", bcolors.NOTIFICATION)\n",
    "analysed_func_dir.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(analysed_func_dir)\n",
    "\n",
    "#Coregistering functional time series and functional signal change map to structural image\n",
    "print_header(\"Coregistering functional time series and functional signal change map to structural image\", bcolors.HEADER)\n",
    "\n",
    "affine_matrix_file = \"mean_func_struct_aligned.aff12.1D\"\n",
    "uncleaned_coregisterd_scm_file = \"signal_change_map_coregistered_structural_space.nii.gz\"\n",
    "coregistered_time_series_file = \"fMRI_coregistered_to_struct.nii.gz\"\n",
    "reference_file = structural_file_for_coregistration\n",
    "input_file1 = mean_image_for_coregistrataion\n",
    "\n",
    "if is_step_enabled(\"coregistration_afni\"):\n",
    "    # Deciding input files based on whether masking was applied or not\n",
    "    if is_step_enabled(\"masking_file\"):\n",
    "        input_file2 = cleaned_scm_file\n",
    "        func_file = func_file_with_cannulas\n",
    "    else:\n",
    "        input_file2 = uncleaned_scm_file\n",
    "        func_file = func_file_without_cannulas\n",
    "        \n",
    "    # Checking if affine matrix already exists\n",
    "    if os.path.exists(affine_matrix_file):\n",
    "        print_statement(\"Affine matrix for SCM coregistration already exists. Reusing it.\", bcolors.OKGREEN)\n",
    "        estimate_affine = False\n",
    "    else:\n",
    "        print_statement(\"Estimating affine matrix for SCM coregistration.\", bcolors.NOTIFICATION)\n",
    "        estimate_affine = True\n",
    "\n",
    "    print_statement(\"Coregistering functional data to structural data using AFNI's 3dAllineate\", bcolors.NOTIFICATION)\n",
    "    coregistration_afni(input_file1, input_file2, reference_file, output_file1=\"mean_func_struct_aligned.nii.gz\", output_file2=uncleaned_coregisterd_scm_file, estimate_affine=estimate_affine, apply_affine=True, affine_mat=affine_matrix_file) # Estimating affnine and applying it to SCM\n",
    "    \n",
    "    print_statement(\"Coregistering functional time series and generating signal change map from coregistered data\", bcolors.NOTIFICATION)\n",
    "    coregistration_afni(input_file1=None, input_file2=func_file, reference_file=reference_file,  output_file1=None, output_file2=coregistered_time_series_file, estimate_affine=False, apply_affine=True, affine_mat=affine_matrix_file) #Applying the same affine to functional time series data\n",
    "\n",
    "    mean = fsl.maths.MeanImage()\n",
    "    mean.inputs.in_file = coregistered_time_series_file\n",
    "    mean.inputs.out_file = f\"mean_{coregistered_time_series_file}\"\n",
    "    mean.run()\n",
    "\n",
    "    if is_step_enabled(\"spatial_smoothing\"):\n",
    "        fwhm_kernel_coregsitered = float(0.1)\n",
    "        \n",
    "        spatial_smoothing(\"fMRI_coregistered_to_struct.nii.gz\", \"sm_fMRI_coregistered_to_struct.nii.gz\", fwhm_kernel_coregsitered)\n",
    "        image_to_be_viewed = \"sm_fMRI_coregistered_to_struct.nii.gz\"\n",
    "        \n",
    "    else:\n",
    "        print_statement(\"⏭️ Spatial Smoothing not executed on coregistered functional data\", bcolors.NOTIFICATION)\n",
    "        image_to_be_viewed = \"fMRI_coregistered_to_struct.nii.gz\"\n",
    "\n",
    "    \n",
    "    if is_step_enabled(\"masking_file\"):\n",
    "        if os.path.exists(f\"mask_mean_{coregistered_time_series_file}\"):\n",
    "            if os.path.exists(f\"cleaned_{image_to_be_viewed}\"):\n",
    "                print_statement(\"Cleaned coregistered functional data exists.\", bcolors.OKGREEN)\n",
    "                cleaned_coregistered_func_ts = f\"cleaned_{image_to_be_viewed}\"\n",
    "            else:\n",
    "                masking_file(image_to_be_viewed, f\"mask_mean_{coregistered_time_series_file}\", f\"cleaned_{image_to_be_viewed}\")\n",
    "                cleaned_coregistered_func_ts = f\"cleaned_{image_to_be_viewed}\"\n",
    "        else:\n",
    "            print_statement(f\"Mask for coregistered functional data does not exist. Creating mask from mean_{coregistered_time_series_file}\", bcolors.NOTIFICATION)\n",
    "            subprocess.run([\"fsleyes\", f\"mean_{coregistered_time_series_file}\"])\n",
    "            masking_file(image_to_be_viewed, f\"mask_mean_{coregistered_time_series_file}\", f\"cleaned_{image_to_be_viewed}\")\n",
    "            cleaned_coregistered_func_ts = f\"cleaned_{image_to_be_viewed}\"\n",
    "    else:\n",
    "        print_statement(\"⏭️ Masks not generated for coregistered functional data, will not get cleaned data in further pipeline\", bcolors.NOTIFICATION)\n",
    "        cleaned_coregistered_func_ts = image_to_be_viewed\n",
    "\n",
    "    #Computing Signal Change Map from coregistered functional time series data\n",
    "\n",
    "    compute_mean_range(input_file=cleaned_coregistered_func_ts, prefix=f\"baseline_sm_fMRI_for_scm_{base_start}_to_{base_end}.nii.gz\", start_idx=base_start, end_idx=base_end)\n",
    "    compute_mean_range(input_file=cleaned_coregistered_func_ts, prefix=f\"signal_sm_fMRI_for_scm_{sig_start}_to_{sig_end}.nii.gz\", start_idx=sig_start, end_idx=sig_end)\n",
    "\n",
    "    signal_change_map(f\"signal_sm_fMRI_for_scm_{sig_start}_to_{sig_end}.nii.gz\", f\"baseline_sm_fMRI_for_scm_{base_start}_to_{base_end}.nii.gz\", f\"sm_coreg_func_Static_Map_{base_start}_to_{base_end}_and_{sig_start}_to_{sig_end}.nii.gz\")\n",
    "\n",
    "    view_images(cleaned_coregistered_func_ts, cmap=\"gray\")\n",
    "    view_images(f\"sm_coreg_func_Static_Map_{base_start}_to_{base_end}_and_{sig_start}_to_{sig_end}.nii.gz\", cmap=\"inferno\")\n",
    "else:\n",
    "    print_statement(\"⏭️ Coregistration not executed, No coregistration_afni step enabled\", bcolors.NOTIFICATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# Marking ROIs and saving time courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Marking ROIs and saving time courses\n",
    "\n",
    "print_header(\"Marking ROIs and saving time courses\", bcolors.HEADER)\n",
    "\n",
    "print_statement(\"Please create ROIs on the functional time series and save them in the following particular format:\", bcolors.NOTIFICATION) \n",
    "print_statement(\"roi_{what protein/aav is there}_{is it direct injection or aav}_{analyte injeted}_{hemisphere side}.nii.gz\", bcolors.NOTIFICATION)\n",
    "print_statement(\"For Example: if GCaMP6f is directly injected in the left hemisphere and dopamine is injected in the right hemisphere following a viral injection, then the following ROIs should be created:\", bcolors.NOTIFICATION) \n",
    "print_statement(\"roi_GCaMP6f_direct_left.nii.gz or roi_dopamine_aav_right.nii.gz\", bcolors.FAIL)  \n",
    "\n",
    "# subprocess.run([\"fsleyes\", \"mean_fMRI_coregistered_to_struct.nii.gz\"])\n",
    "print(analysed_func_dir)\n",
    "\n",
    "\n",
    "files_list_roi = os.listdir(analysed_func_dir)\n",
    "\n",
    "for file in files_list_roi:\n",
    "    if file.startswith(\"roi_\") and file.endswith(\"left.nii.gz\"):\n",
    "        roi_left = file\n",
    "    if file.startswith(\"roi_\") and file.endswith(\"right.nii.gz\"):\n",
    "        roi_right = file\n",
    "\n",
    "mean_signal_left = roi_analysis(\"fMRI_coregistered_to_struct.nii.gz\", roi_left, n_vols, tr, base_start, base_end, sig_start, sig_end)\n",
    "mean_signal_right = roi_analysis(\"fMRI_coregistered_to_struct.nii.gz\", roi_right, n_vols, tr, base_start, base_end, sig_start, sig_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# Voxel-wise Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the dropdown select the file you want to test the correlation with\n",
    "fc = FileChooser(analysis_dir, layout=widgets.Layout(width='1080px'))\n",
    "fc.show_only_dirs = False\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the selected file above using nibabel and then select for mask file and seed file\n",
    "func_file_for_correlation = fc.selected_filename\n",
    "print(func_file_for_correlation)\n",
    "img_func = nib.load(func_file_for_correlation)\n",
    "img_func.shape\n",
    "\n",
    "matches = matching_nifti_files(cwd, img_func.shape)\n",
    "dropdown_mask = widgets.Dropdown(options=matches, description='Mask file:', layout=widgets.Layout(width='40%'))\n",
    "display(dropdown_mask)\n",
    "\n",
    "dropdown_seed = widgets.Dropdown(options=matches, description='Seed file:', layout=widgets.Layout(width='40%'))\n",
    "display(dropdown_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data and extract voxel wise time series and save it\n",
    "mask_file_for_correlation = os.path.join(cwd, dropdown_mask.value)\n",
    "seed_file_for_correlation = os.path.join(cwd, dropdown_seed.value)\n",
    "\n",
    "\n",
    "var_func = nib.load(func_file_for_correlation).get_fdata()\n",
    "var_seed = nib.load(seed_file_for_correlation).get_fdata().astype(bool)\n",
    "var_mask = nib.load(mask_file_for_correlation).get_fdata().astype(bool)\n",
    "\n",
    "ts_seed = var_func[var_seed, :].T   # (time, voxels)\n",
    "print(\"Seed TS:\", ts_seed.shape)\n",
    "\n",
    "ts_mask = var_func[var_mask, :].T   # (time, voxels)\n",
    "print(\"Mask TS:\", ts_mask.shape)\n",
    "\n",
    "np.savetxt(\"roi_voxel_seed_timeseries.txt\", ts_seed, fmt=\"%.6f\", delimiter=\"\\t\")\n",
    "np.savetxt(\"roi_voxel_mask_timeseries.txt\", ts_mask, fmt=\"%.6f\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Parameters for selecting start and end index for correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layout = widgets.Layout(width=\"280px\", flex=\"0 0 auto\")\n",
    "label_style = {'description_width': '410px'}\n",
    "\n",
    "# ---------- Widgets ----------\n",
    "start_idx_correlation = widgets.IntText(value=1300, description=\"Start Index for Correlation: \", layout=input_layout, style=label_style)\n",
    "stop_idx_correlation = widgets.IntText(value=2300, description=\"End Index for Correlation: \", layout=input_layout, style=label_style)\n",
    "\n",
    "for w in [start_idx_correlation, stop_idx_correlation]:\n",
    "    w.style.description_width = \"180px\"\n",
    "\n",
    "# ---------- Container ----------\n",
    "row = widgets.HBox([start_idx_correlation, stop_idx_correlation],\n",
    "    layout=widgets.Layout(width=\"40%\", justify_content=\"space-between\", align_items=\"center\", padding=\"12px\", border=\"3px solid #444\", border_radius=\"10px\", background_color=\"#1e1e1e\"))\n",
    "\n",
    "row.layout.background_color = \"#1e1e1e\"\n",
    "row.layout.border = \"3px solid #444\"\n",
    "\n",
    "display(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = start_idx_correlation.value\n",
    "stop = stop_idx_correlation.value\n",
    "# ============================================================\n",
    "# Save voxel indices\n",
    "# ============================================================\n",
    "voxel_indices_seed = np.column_stack(np.where(var_seed))\n",
    "voxel_indices_mask = np.column_stack(np.where(var_mask))\n",
    "\n",
    "np.savetxt(\"roi_voxel_indices_seed.txt\", voxel_indices_seed, fmt=\"%d\")\n",
    "np.savetxt(\"roi_voxel_indices_mask.txt\", voxel_indices_mask, fmt=\"%d\")\n",
    "\n",
    "# ============================================================\n",
    "# Mean seed time series\n",
    "# ============================================================\n",
    "mean_ts_seed = ts_seed.mean(axis=1)\n",
    "np.savetxt(\"roi_mean_seed_timeseries.txt\", mean_ts_seed, fmt=\"%.6f\")\n",
    "\n",
    "# Debug slice\n",
    "np.savetxt(\"test.txt\", mean_ts_seed[start:stop])\n",
    "\n",
    "# ============================================================\n",
    "# Correlation computation (TIME DOMAIN)\n",
    "# ============================================================\n",
    "correlation_values = np.zeros((2, ts_mask.shape[1]), dtype=np.float32)\n",
    "\n",
    "x_ts = mean_ts_seed[start:stop]   # TIME series (length = stop-start)\n",
    "\n",
    "for col in range(ts_mask.shape[1]):\n",
    "    y_ts = ts_mask[start:stop, col]\n",
    "    r, p = scipy.stats.pearsonr(x_ts, y_ts)\n",
    "    correlation_values[0, col] = r\n",
    "    correlation_values[1, col] = p\n",
    "\n",
    "# ============================================================\n",
    "# FDR inputs\n",
    "# ============================================================\n",
    "rvals = correlation_values[0]\n",
    "pvals = correlation_values[1]\n",
    "\n",
    "# ============================================================\n",
    "# Fisher z-transform\n",
    "# ============================================================\n",
    "eps = np.finfo(np.float32).eps\n",
    "rvals_clipped = np.clip(rvals, -1 + eps, 1 - eps)\n",
    "zvals = np.arctanh(rvals_clipped)\n",
    "\n",
    "# ============================================================\n",
    "# Save EVERYTHING in one file\n",
    "# ============================================================\n",
    "out = np.column_stack((voxel_indices_mask, rvals, pvals, zvals))\n",
    "\n",
    "np.savetxt(\"seed_voxelwise_correlation.txt\", out, fmt=\"%d\\t%d\\t%d\\t%.6f\\t%.6e\\t%.6f\", header=\"x\\ty\\tz\\tr\\tp\\tz_fisher\")\n",
    "\n",
    "print(\"Rows written:\", out.shape[0])\n",
    "print(\"Mask voxels:\", np.sum(var_mask))\n",
    "\n",
    "# ============================================================\n",
    "# -------------------- PLOTTING -------------------------------\n",
    "# ============================================================\n",
    "data = np.loadtxt(\"seed_voxelwise_correlation.txt\")\n",
    "# ---------------------------\n",
    "# Matplotlib style\n",
    "# ---------------------------\n",
    "plt.rcParams['font.family'] = ['serif']\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 14\n",
    "plt.rcParams[\"xtick.labelsize\"] = 12\n",
    "plt.rcParams[\"ytick.labelsize\"] = 12\n",
    "plt.rcParams[\"legend.fontsize\"] = 12\n",
    "\n",
    "# ---------------------------\n",
    "# Data (VOXEL DOMAIN)\n",
    "# ---------------------------\n",
    "group_ids = data[:, 1].astype(int)   # voxel groups\n",
    "y_all     = data[:, 3]               # correlation coefficients\n",
    "\n",
    "assert y_all.shape[0] == group_ids.shape[0], \"Voxel data mismatch\"\n",
    "\n",
    "x_vox = np.arange(y_all.shape[0])    # voxel index axis\n",
    "unique_groups = np.unique(group_ids)\n",
    "\n",
    "# ---------------------------\n",
    "# Figure + GridSpec\n",
    "# ---------------------------\n",
    "fig = plt.figure(figsize=(24, 14))\n",
    "gs = GridSpec(nrows=3, ncols=8, height_ratios=[2.5, 1.5, 1.5], hspace=0.4)\n",
    "\n",
    "# ============================================================\n",
    "# Helper function: scatter by correlation range\n",
    "# ============================================================\n",
    "def scatter_by_range(ax, x, y):\n",
    "    mask_zero        = (y == 0)\n",
    "    mask_red         = (y > 0.5)  & (y <= 1.0)\n",
    "    mask_orange      = (y > 0.0)  & (y <= 0.5)\n",
    "    mask_greenyellow = (y >= -0.5) & (y < 0.0)\n",
    "    mask_blue        = (y >= -1.0) & (y < -0.5)\n",
    "\n",
    "    ax.scatter(x[mask_zero],        y[mask_zero],        color=\"black\",       s=18, label=\"0\")\n",
    "    ax.scatter(x[mask_red],         y[mask_red],         color=\"red\",         s=18, label=\"0.5 to 1.0\")\n",
    "    ax.scatter(x[mask_orange],      y[mask_orange],      color=\"orange\",      s=18, label=\"0 to 0.5\")\n",
    "    ax.scatter(x[mask_greenyellow], y[mask_greenyellow], color=\"greenyellow\", s=18, label=\"-0.5 to 0\")\n",
    "    ax.scatter(x[mask_blue],        y[mask_blue],        color=\"blue\",        s=18, label=\"-1 to -0.5\")\n",
    "\n",
    "# ============================================================\n",
    "# TOP: Combined plot (ALL voxels)\n",
    "# ============================================================\n",
    "ax_top = fig.add_subplot(gs[0, :])\n",
    "\n",
    "scatter_by_range(ax_top, x_vox, y_all)\n",
    "\n",
    "ax_top.set_title(\"Voxel-wise Correlation Coefficient — All Slices\")\n",
    "ax_top.set_ylabel(\"Correlation Coefficient\")\n",
    "ax_top.set_xlabel(\"Voxel Index\")\n",
    "\n",
    "handles, labels = ax_top.get_legend_handles_labels()\n",
    "ax_top.legend(handles, labels, ncol=5, frameon=False, loc=\"upper right\")\n",
    "\n",
    "# ============================================================\n",
    "# BOTTOM: Grouped subplots (8 × 2)\n",
    "# ============================================================\n",
    "for idx, g in enumerate(unique_groups[:16]):  # max 16 subplots\n",
    "    row = 1 + idx // 8\n",
    "    col = idx % 8\n",
    "\n",
    "    ax = fig.add_subplot(gs[row, col], sharex=ax_top, sharey=ax_top)\n",
    "\n",
    "    group_mask = (group_ids == g)\n",
    "    x_g = x_vox[group_mask]\n",
    "    y_g = y_all[group_mask]\n",
    "\n",
    "    scatter_by_range(ax, x_g, y_g)\n",
    "\n",
    "    ax.set_title(f\"group = {g}\", fontsize=12)\n",
    "\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", bottom=False, top=False, labelbottom=False)\n",
    "    ax.tick_params(axis=\"y\", labelsize=10)\n",
    "\n",
    "# ---------------------------\n",
    "# Save figure\n",
    "# ---------------------------\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"corr_coeff_combined_and_grouped.svg\", dpi=1200)\n",
    "# plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "# Entering all data analysis record in the SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to a new SQlite database\n",
    "db_name = \"analysis_record_test.db\"\n",
    "db = os.path.join(root_location, db_name)\n",
    "conn = sqlite3.connect(db)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#drop old table if it exists\n",
    "# cursor.execute(\"DROP TABLE IF EXISTS Data_Analysis\")\n",
    "conn.commit()\n",
    "\n",
    "user = getpass.getuser()\n",
    "\n",
    "# This is validation step, if any value is missing that indicates that the previous steps have not been successfully implemented. \n",
    "required_fields = {\n",
    "    \"analysis_done_by\": user,\n",
    "    \"root_location\": root_location,\n",
    "    \"subject_id\": subject_id,\n",
    "    \"analysis_folder_name\": folder_created,\n",
    "    \"functional_run\": func_scan_number,\n",
    "    \"structural_run\": struct_scan_number,\n",
    "    \"temporal_res\": tr,\n",
    "    \"window_duration\": win_entered.value,\n",
    "    \"spatial_smoothing\": fwhm_kernel,\n",
    "    \"spatial_smoothing_coreg_image\": fwhm_kernel_coregsitered,\n",
    "    \"roi_left\": roi_left,\n",
    "    \"mean_signal_change_left\": mean_signal_left,\n",
    "    \"roi_right\": roi_right,\n",
    "    \"mean_signal_change_right\": mean_signal_right,\n",
    "    \"start_idx_correlation\": start_idx_correlation.value,\n",
    "    \"end_idx_correlation\": stop_idx_correlation.value,\n",
    "}\n",
    "\n",
    "def validate_analysis_inputs(fields):\n",
    "    missing = [\n",
    "        name for name, value in fields.items()\n",
    "        if value is None or (isinstance(value, str) and value.strip() == \"\")\n",
    "    ]\n",
    "\n",
    "    if missing:\n",
    "        raise RuntimeError(\n",
    "            \"Analysis NOT COMPLETE, Please run your complete pipeline for successful record.\\n\"\n",
    "            f\"Missing fields: {', '.join(missing)}\"\n",
    "        )\n",
    "\n",
    "    if tr <= 0:\n",
    "        raise RuntimeError(\"Invalid temporal resolution (TR must be > 0)\")\n",
    "\n",
    "    if win_entered.value <= 0:\n",
    "        raise RuntimeError(\"Invalid window duration\")\n",
    "\n",
    "    if start_idx_correlation.value >= stop_idx_correlation.value:\n",
    "        raise RuntimeError(\"Invalid correlation window indices\")\n",
    "\n",
    "# Run validation\n",
    "validate_analysis_inputs(required_fields)\n",
    "\n",
    "# Here we are creating the table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Data_Analysis (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    analysis_done_by TEXT,\n",
    "    root_location TEXT,\n",
    "    subject_id TEXT,\n",
    "    analysis_folder_name TEXT,\n",
    "    functional_run INTEGER,\n",
    "    structural_run INTEGER,\n",
    "    temporal_res REAL,\n",
    "    window_duration INTEGER,\n",
    "    spatial_smoothing REAL,\n",
    "    spatial_smoothing_coreg_image REAL,\n",
    "    roi_left TEXT,\n",
    "    mean_signal_change_left FLOAT,\n",
    "    roi_right TEXT,\n",
    "    mean_signal_change_right FLOAT,\n",
    "    start_idx_correlation INTEGER,\n",
    "    end_idx_correlation INTEGER,\n",
    "    created_at TEXT DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Based on the parameters used to analyse data throughout, enter them in the SQL database to create a record of what parameters were used for which data\n",
    "Data_Analysis = tuple(required_fields.values())\n",
    "cursor.execute(\"\"\"\n",
    "INSERT INTO Data_Analysis (analysis_done_by,root_location, subject_id, analysis_folder_name, functional_run, structural_run, temporal_res, window_duration, spatial_smoothing, \n",
    "               spatial_smoothing_coreg_image, roi_left, mean_signal_change_left, roi_right, mean_signal_change_right, start_idx_correlation, end_idx_correlation)\n",
    "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\", Data_Analysis)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we get the print of the data that we entered\n",
    "df = pd.read_sql(\"SELECT * FROM Data_Analysis ORDER BY id DESC\", con=conn)\n",
    "display(df[df['analysis_done_by'] == user])\n",
    "\n",
    "# conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
